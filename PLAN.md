# –ü–ª–∞–Ω –ø–µ—Ä–µ–≤–æ–¥–∞ —Å PPO –Ω–∞ SAC (Soft Actor-Critic)

## –û–±–∑–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π

–ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç PPO (Proximal Policy Optimization) –∫ SAC (Soft Actor-Critic) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ multi-agent –æ–∫—Ä—É–∂–µ–Ω–∏–∏ —Å —Ç–∞–Ω–∫–∞–º–∏.

---

## 1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–µ—Ç–µ–π

### 1.1. Policy Network (Actor)

**–¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (PPO):**
- –í—ã—Ö–æ–¥—ã: `mean` –∏ `log_std` –¥–ª—è Gaussian policy
- –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º ratio

**–ò–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è SAC:**
- ‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å –≤—ã—Ö–æ–¥—ã `mean` –∏ `log_std`
- ‚ûï –î–æ–±–∞–≤–∏—Ç—å reparameterization trick: `action = mean + std * noise`
- ‚ûï –ü—Ä–∏–º–µ–Ω–∏—Ç—å `tanh` squashing –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –≤ [-1, 1]
- ‚ûï –í—ã—á–∏—Å–ª—è—Ç—å log-probability —Å –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –Ω–∞ tanh:
  ```
  log_œÄ(a|s) = log_œÄ(u|s) - Œ£ log(1 - tanh¬≤(u))
  ```
- üóëÔ∏è –£–±—Ä–∞—Ç—å –∫–ª–∏–ø–ø–∏–Ω–≥ PPO (clip ratio)

**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml/src/Models/Create.ts` - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å `createPolicyNetwork()`

### 1.2. Critic Networks (Q-functions)

**–¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (PPO):**
- –û–¥–Ω–∞ Value Network: `V(s)` - –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ

**–ò–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è SAC:**
- üóëÔ∏è –£–¥–∞–ª–∏—Ç—å `createValueNetwork()`
- ‚ûï –°–æ–∑–¥–∞—Ç—å **–¥–≤–µ Q-Networks**: `Q1(s,a)` –∏ `Q2(s,a)` (twin Q-networks –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è overestimation bias)
- ‚ûï –°–æ–∑–¥–∞—Ç—å **–¥–≤–µ Target Q-Networks**: `Q1_target` –∏ `Q2_target` (–¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è)
- ‚ûï –í—Ö–æ–¥—ã: state + action (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –ø–æ—Å–ª–µ encoding —Å–æ—Å—Ç–æ—è–Ω–∏—è)
- ‚ûï –í—ã—Ö–æ–¥: —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Q(s,a)

**–§–∞–π–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è:**
- `packages/ml/src/Models/Create.ts` - –¥–æ–±–∞–≤–∏—Ç—å `createCriticNetwork()`

### 1.3. Temperature Parameter (Œ±)

- ‚ûï –î–æ–±–∞–≤–∏—Ç—å learnable —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä `Œ±` (entropy coefficient)
- ‚ûï –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞ Œ± –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏
- ‚ûï –¶–µ–ª–µ–≤–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è: `-dim(action_space)` = `-ACTION_DIM`

**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml/src/SAC/train.ts` - –¥–æ–±–∞–≤–∏—Ç—å `trainTemperature()`

---

## 2. –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ Replay Buffer

### 2.1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

**–¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (PPO):**
```typescript
{
  states,        // —Å–æ—Å—Ç–æ—è–Ω–∏—è
  actions,       // –¥–µ–π—Å—Ç–≤–∏—è
  rewards,       // –Ω–∞–≥—Ä–∞–¥—ã
  oldLogProbs,   // —Å—Ç–∞—Ä—ã–µ log-probabilities
  oldValues,     // —Å—Ç–∞—Ä—ã–µ value estimates
  advantages,    // –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ (GAE)
  returns        // –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ returns (V-trace)
}
```

**–î–ª—è SAC:**
```typescript
{
  states,        // —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ s_t
  actions,       // –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ a_t
  rewards,       // –ø–æ–ª—É—á–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ r_t
  next_states,   // —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ s_{t+1}
  dones          // —Ñ–ª–∞–≥ –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞
}
```

### 2.2. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª

- üóëÔ∏è –£–±—Ä–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ `advantages` –∏ `returns` (GAE/V-trace –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è SAC)
- üóëÔ∏è –£–±—Ä–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ `oldLogProbs`, `oldValues`
- ‚ûï –î–æ–±–∞–≤–∏—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏–µ `next_states` –∏ `dones`
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π `ReplayBuffer` —Å uniform sampling
- ‚ûï **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ `PrioritizedReplayBuffer` (—É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ `packages/ml-common/PrioritizedReplayBuffer.ts`)

**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml-common/Memory.ts` - –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É `AgentMemoryBatch`
- `packages/ml/src/SAC/Actor/EpisodeManager.ts` - –∏–∑–º–µ–Ω–∏—Ç—å –ª–æ–≥–∏–∫—É —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

---

## 3. –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏

### 3.1. –§–∞–π–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

**–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É:**
```
packages/ml/src/
  SAC/                              # –Ω–æ–≤–∞—è –ø–∞–ø–∫–∞ (–≤–º–µ—Å—Ç–æ PPO/)
    index.ts                        # —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
    train.ts                        # —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    ActorWorker.ts                  # worker –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    LearnerActorWorker.ts           # worker –¥–ª—è –æ–±—É—á–µ–Ω–∏—è policy
    LearnerCriticWorker.ts          # worker –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Q-functions
    channels.ts                     # –∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏ –º–µ–∂–¥—É workers
    Actor/
      EpisodeManager.ts             # —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞–º–∏ –∏ —Å–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö
    Learner/
      createLearnerManager.ts       # –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
      createActorLearner.ts         # –æ–±—É—á–µ–Ω–∏–µ actor (policy)
      createCriticLearner.ts        # –æ–±—É—á–µ–Ω–∏–µ critics (Q-functions)
      isLossDangerous.ts            # –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ loss
    VisTest/
      VisTestEpisodeManager.ts      # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```

### 3.2. Training Loop - Actor (Policy)

**–£–¥–∞–ª–∏—Ç—å:**
```typescript
trainPolicyNetwork(
  network, states, actions, oldLogProbs, 
  advantages, clipRatio, entropyCoeff
)
```

**–î–æ–±–∞–≤–∏—Ç—å:**
```typescript
trainActorNetwork(
  actorNetwork: tf.LayersModel,
  criticNetwork1: tf.LayersModel,
  criticNetwork2: tf.LayersModel,
  states: tf.Tensor[],
  alpha: number | tf.Variable,  // temperature parameter
  batchSize: number,
  clipNorm: number,
  minLogStd: number,
  maxLogStd: number,
  returnCost: boolean,
): tf.Tensor | undefined {
  return tf.tidy(() => {
    return optimize(actorNetwork.optimizer, () => {
      // 1. Sample actions from current policy with reparameterization
      const { action, logProb } = sampleActionWithReparam(
        actorNetwork, states, minLogStd, maxLogStd
      );
      
      // 2. Compute Q-values from both critics
      const q1 = criticNetwork1.predict([...states, action], { batchSize });
      const q2 = criticNetwork2.predict([...states, action], { batchSize });
      const minQ = tf.minimum(q1, q2).squeeze();
      
      // 3. Actor loss: E[Œ± * log œÄ(a|s) - Q(s,a)]
      // Maximize Q - entropy bonus
      const alphaValue = typeof alpha === 'number' ? alpha : alpha.read();
      const actorLoss = tf.scalar(alphaValue).mul(logProb).sub(minQ).mean();
      
      return actorLoss as tf.Scalar;
    }, { clipNorm, returnCost });
  });
}
```

### 3.3. Training Loop - Critic (Q-functions)

**–£–¥–∞–ª–∏—Ç—å:**
```typescript
trainValueNetwork(
  network, states, returns, oldValues, clipRatio
)
```

**–î–æ–±–∞–≤–∏—Ç—å:**
```typescript
trainCriticNetworks(
  critic1: tf.LayersModel,
  critic2: tf.LayersModel,
  targetCritic1: tf.LayersModel,
  targetCritic2: tf.LayersModel,
  actorNetwork: tf.LayersModel,
  batch: {
    states: tf.Tensor[],
    actions: tf.Tensor,
    rewards: tf.Tensor,
    nextStates: tf.Tensor[],
    dones: tf.Tensor
  },
  alpha: number | tf.Variable,
  gamma: number,
  batchSize: number,
  clipNorm: number,
  minLogStd: number,
  maxLogStd: number,
  returnCost: boolean,
): { loss1: tf.Tensor | undefined, loss2: tf.Tensor | undefined } {
  return tf.tidy(() => {
    // 1. Sample next actions from current policy
    const { action: nextAction, logProb: nextLogProb } = 
      sampleActionWithReparam(
        actorNetwork, batch.nextStates, minLogStd, maxLogStd
      );
    
    // 2. Compute target Q-value with entropy regularization
    const targetQ1 = targetCritic1.predict(
      [...batch.nextStates, nextAction], { batchSize }
    ).squeeze();
    const targetQ2 = targetCritic2.predict(
      [...batch.nextStates, nextAction], { batchSize }
    ).squeeze();
    const minTargetQ = tf.minimum(targetQ1, targetQ2);
    
    const alphaValue = typeof alpha === 'number' ? alpha : alpha.read();
    const target = batch.rewards.add(
      tf.scalar(gamma).mul(
        tf.scalar(1).sub(batch.dones).mul(
          minTargetQ.sub(tf.scalar(alphaValue).mul(nextLogProb))
        )
      )
    );
    const targetDetached = target.stopGradient();
    
    // 3. Compute current Q-values
    const currentQ1 = critic1.predict(
      [...batch.states, batch.actions], { batchSize }
    ).squeeze();
    const currentQ2 = critic2.predict(
      [...batch.states, batch.actions], { batchSize }
    ).squeeze();
    
    // 4. MSE loss for both critics
    const loss1 = optimize(critic1.optimizer, () => {
      return tf.losses.meanSquaredError(targetDetached, currentQ1)
        .mean() as tf.Scalar;
    }, { clipNorm, returnCost });
    
    const loss2 = optimize(critic2.optimizer, () => {
      return tf.losses.meanSquaredError(targetDetached, currentQ2)
        .mean() as tf.Scalar;
    }, { clipNorm, returnCost });
    
    return { loss1, loss2 };
  });
}
```

### 3.4. Temperature (Œ±) Update

```typescript
trainTemperature(
  alpha: tf.Variable,
  logProbs: tf.Tensor,
  targetEntropy: number,  // –æ–±—ã—á–Ω–æ -dim(action_space) = -ACTION_DIM
  learningRate: number,
  clipNorm: number,
  returnCost: boolean,
): tf.Tensor | undefined {
  return tf.tidy(() => {
    // Alpha loss: -Œ± * (log œÄ + target_entropy)
    // Minimize: maximize entropy when it's below target
    return optimize(
      new tf.train.adam(learningRate), 
      () => {
        const alphaLoss = alpha.mul(
          logProbs.add(tf.scalar(targetEntropy))
        ).mean().mul(-1);
        return alphaLoss as tf.Scalar;
      },
      { clipNorm, returnCost }
    );
  });
}
```

### 3.5. Soft Target Update

```typescript
softUpdateTargetNetworks(
  critic1: tf.LayersModel,
  critic2: tf.LayersModel,
  targetCritic1: tf.LayersModel,
  targetCritic2: tf.LayersModel,
  tau: number = 0.005,  // soft update coefficient
): void {
  // Polyak averaging: Œ∏_target = œÑ * Œ∏ + (1 - œÑ) * Œ∏_target
  updateWeightsSoft(critic1, targetCritic1, tau);
  updateWeightsSoft(critic2, targetCritic2, tau);
}

function updateWeightsSoft(
  source: tf.LayersModel,
  target: tf.LayersModel,
  tau: number,
): void {
  const sourceWeights = source.getWeights();
  const targetWeights = target.getWeights();
  
  const updatedWeights = targetWeights.map((targetWeight, i) => {
    return tf.tidy(() => {
      const sourceWeight = sourceWeights[i];
      return sourceWeight.mul(tau).add(
        targetWeight.mul(1 - tau)
      );
    });
  });
  
  target.setWeights(updatedWeights);
  
  // Dispose old weights
  targetWeights.forEach(w => w.dispose());
}
```

**–§–∞–π–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è/–∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml/src/SAC/train.ts` - –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
- `packages/ml/src/Models/Utils.ts` - –¥–æ–±–∞–≤–∏—Ç—å `updateWeightsSoft()`

---

## 4. Actor (—Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö)

### 4.1. EpisodeManager

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**
- üóëÔ∏è –£–±—Ä–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ GAE (Generalized Advantage Estimation)
- üóëÔ∏è –£–±—Ä–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ V-trace
- üóëÔ∏è –£–±—Ä–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ `oldValues` –∏ `oldLogProbs`
- ‚ûï –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–µ—Ä–µ—Ö–æ–¥—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ `(s, a, r, s', done)` –≤–º–µ—Å—Ç–æ `(s, a, r, value, logProb)`
- ‚ûï –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è `next_state`
- ‚ûï –î–æ–±–∞–≤–∏—Ç—å —Ñ–ª–∞–≥ `done` –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
- ‚ûï –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–æ–±–∞–≤–∏—Ç—å exploration noise –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö (Ornstein-Uhlenbeck –∏–ª–∏ Gaussian)

**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml/src/SAC/Actor/EpisodeManager.ts`
- `packages/ml-common/Memory.ts`

### 4.2. Inference

- ‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π –≤–æ –≤—Ä–µ–º—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
- ‚ûï –ü—Ä–∏–º–µ–Ω—è—Ç—å `tanh` squashing –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π
- ‚ûï –î–ª—è evaluation/testing: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `mean` –±–µ–∑ —à—É–º–∞ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞)
- ‚ûï Reparameterization trick –¥–ª—è backprop —á–µ—Ä–µ–∑ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml/src/SAC/Actor/EpisodeManager.ts`
- `packages/ml-common/computeLogProb.ts` - –¥–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Å tanh correction

---

## 5. –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏

### 5.1. Create.ts - Critic Network

```typescript
export function createCriticNetwork(): tf.LayersModel {
  // State inputs (–∫–∞–∫ –≤ policy network)
  const {
    controllerInput,
    battleInput,
    tankInput,
    enemiesInput,
    alliesInput,
    bulletsInput,
  } = createInputs();
  
  // Action input
  const actionInput = tf.input({
    shape: [ACTION_DIM],
    name: 'action_input',
    dtype: 'float32',
  });
  
  // Encode states —á–µ—Ä–µ–∑ transformer (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏–∫—É –∏–∑ policy)
  const tokens = convertInputsToTokens(
    Model.Critic1, // –∏–ª–∏ Critic2
    controllerInput,
    battleInput,
    tankInput,
    enemiesInput,
    alliesInput,
    bulletsInput,
    criticNetworkConfig.dim,
  );
  
  const transformed = applySelfTransformLayers(
    Model.Critic1,
    tokens,
    criticNetworkConfig.dim,
    criticNetworkConfig.heads,
    criticNetworkConfig.dropout,
  );
  
  // Global pooling –¥–ª—è state representation
  const stateEncoding = tf.layers.globalAveragePooling1d({
    name: Model.Critic1 + '_pool',
  }).apply(transformed) as tf.SymbolicTensor;
  
  // Concatenate state encoding + action
  const combined = tf.layers.concatenate({
    name: Model.Critic1 + '_concat',
  }).apply([stateEncoding, actionInput]) as tf.SymbolicTensor;
  
  // MLP layers
  const hidden = applyDenseLayers(
    Model.Critic1 + '_mlp',
    combined,
    criticNetworkConfig.finalMLP,
  );
  
  // Q-value output (single scalar)
  const qValue = tf.layers.dense({
    name: Model.Critic1 + '_q_value',
    units: 1,
    activation: 'linear',
  }).apply(hidden) as tf.SymbolicTensor;
  
  const model = tf.model({
    inputs: [
      controllerInput,
      battleInput,
      tankInput,
      enemiesInput,
      alliesInput,
      bulletsInput,
      actionInput,
    ],
    outputs: qValue,
    name: Model.Critic1,
  });
  
  model.compile({
    optimizer: new PatchedAdamOptimizer(CONFIG.lr),
    loss: 'meanSquaredError',
  });
  
  return model;
}
```

### 5.2. Model Enum

```typescript
// packages/ml/src/Models/def.ts
export enum Model {
  Policy = 'policy',        // –∏–ª–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –≤ Actor
  Critic1 = 'critic1',
  Critic2 = 'critic2',
  TargetCritic1 = 'target_critic1',
  TargetCritic2 = 'target_critic2',
}
```

### 5.3. Network Config

```typescript
// packages/ml/src/Models/Create.ts
const criticNetworkConfig: NetworkConfig = {
  dim: 64,               // —Ä–∞–∑–º–µ—Ä embeddings
  heads: 4,              // –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ attention heads
  dropout: 0.0,          // dropout rate
  finalMLP: [
    ['relu', 256],
    ['relu', 256],
    ['relu', 128],
  ] as [ActivationIdentifier, number][],
};
```

**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml/src/Models/Create.ts`
- `packages/ml/src/Models/def.ts`

---

## 6. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã

### 6.1. –£–¥–∞–ª–∏—Ç—å (PPO-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ)

- ‚ùå `clipRatio` (0.2) - PPO clipping
- ‚ùå `entropyCoeff` (0.01) - –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ `alpha`
- ‚ùå `vfCoeff` (0.5) - value function coefficient
- ‚ùå GAE –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (`lambda`, `gamma` –¥–ª—è GAE)

### 6.2. –î–æ–±–∞–≤–∏—Ç—å (SAC-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ)

```typescript
// packages/ml-common/config.ts
export const SAC_CONFIG = {
  // Temperature
  alpha: 0.2,                    // entropy coefficient (–∏–ª–∏ auto-tune)
  autoTuneAlpha: true,           // –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞
  targetEntropy: -ACTION_DIM,    // —Ü–µ–ª–µ–≤–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è auto-tune
  alphaLR: 3e-4,                 // learning rate –¥–ª—è alpha
  
  // Soft target update
  tau: 0.005,                    // –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç Polyak averaging
  
  // Discount factor
  gamma: 0.99,                   // discount factor
  
  // Replay buffer
  replayBufferSize: 1_000_000,   // —Ä–∞–∑–º–µ—Ä replay buffer
  prioritizedReplay: true,       // –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PER
  priorityAlpha: 0.6,            // –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è
  priorityBeta: 0.4,             // importance sampling
  
  // Training
  batchSize: 256,                // —Ä–∞–∑–º–µ—Ä batch
  actorUpdateFreq: 1,            // —á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è actor
  criticUpdateFreq: 1,           // —á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è critics
  targetUpdateFreq: 1,           // —á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è target networks
  
  // Learning rates
  actorLR: 3e-4,                 // learning rate –¥–ª—è actor
  criticLR: 3e-4,                // learning rate –¥–ª—è critics
  
  // Gradient clipping
  clipNorm: 1.0,                 // gradient clipping norm
  
  // Exploration
  initialRandomSteps: 10000,     // –Ω–∞—á–∞–ª—å–Ω—ã–µ random —à–∞–≥–∏
  
  // Log std bounds
  minLogStd: -20,                // –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ log std
  maxLogStd: 2,                  // –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ log std
};
```

**–§–∞–π–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è/–∏–∑–º–µ–Ω–µ–Ω–∏—è:**
- `packages/ml-common/config.ts` - –¥–æ–±–∞–≤–∏—Ç—å `SAC_CONFIG`

---

## 7. –ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –º–∏–≥—Ä–∞—Ü–∏–∏

### –§–∞–∑–∞ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ (1-2 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –ò–∑—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É PPO
2. ‚úÖ –°–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É `feat/sac`
3. ‚úÖ –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É `packages/ml/src/SAC/`
4. ‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ `PPO/`
5. ‚úÖ –°–æ–∑–¥–∞—Ç—å `SAC_CONFIG` –≤ `packages/ml-common/config.ts`

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫ —Å–æ–∑–¥–∞–Ω–∞
- ‚úÖ –ë–∞–∑–æ–≤—ã–µ —Ñ–∞–π–ª—ã —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã
- ‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞

### –§–∞–∑–∞ 2: –ú–æ–¥–µ–ª–∏ (2-3 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å `createCriticNetwork()` –≤ `packages/ml/src/Models/Create.ts`
2. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å `Critic1`, `Critic2`, `TargetCritic1`, `TargetCritic2` –≤ `Model` enum
3. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `softUpdateTargetNetwork()` –¥–ª—è soft target update
4. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å `sampleActionWithTanhSquashing()` —Å tanh squashing –∏ log-prob correction
5. ‚¨ú –°–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `sampleActionWithReparam()` —Å reparameterization trick
6. ‚¨ú –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å forward pass –≤—Å–µ—Ö —Å–µ—Ç–µ–π
7. ‚¨ú –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–æ–≤/–≤—ã—Ö–æ–¥–æ–≤

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ: –í—Å–µ 4 —Å–µ—Ç–∏ (Actor, Critic1, Critic2, Target Critics) —Å–æ–∑–¥–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
- ‚¨ú Forward pass —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- ‚úÖ Soft update —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω

**–§–∞–π–ª—ã:**
- ‚úÖ `packages/ml/src/Models/Create.ts`
- ‚úÖ `packages/ml/src/Models/def.ts`
- ‚úÖ `packages/ml/src/Models/Utils.ts`
- ‚úÖ `packages/ml-common/computeLogProb.ts`

### –§–∞–∑–∞ 3: Training (3-4 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/train.ts`
2. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `trainCriticNetworks()`
3. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `trainActorNetwork()`
4. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `trainTemperature()` –¥–ª—è auto-tuning alpha
5. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å helper —Ñ—É–Ω–∫—Ü–∏–∏ (`parsePredict`, `optimize`)
6. ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `act()` –¥–ª—è inference —Å deterministic —Ä–µ–∂–∏–º–æ–º
7. ‚¨ú –£–±—Ä–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ PPO: `trainPolicyNetwork()`, `trainValueNetwork()` (–æ—Å—Ç–∞–≤–∏–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
8. ‚¨ú –£–±—Ä–∞—Ç—å V-trace/GAE: `computeVTrace()`, `computeGAE()` (–Ω–µ –Ω—É–∂–Ω–æ —Ç—Ä–æ–≥–∞—Ç—å PPO)
9. ‚¨ú –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã
- ‚¨ú Loss –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö (–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –≤ –§–∞–∑–µ 8)
- ‚¨ú –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è NaN/Inf (–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –≤ –§–∞–∑–µ 8)

**–§–∞–π–ª—ã:**
- ‚úÖ `packages/ml/src/SAC/train.ts`

### –§–∞–∑–∞ 4: Replay Buffer (1-2 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `AgentMemoryBatch` –≤ `packages/ml-common/Memory.ts`:
   - ‚úÖ –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ç–∏–ø `SACMemoryBatch`
   - ‚úÖ –î–æ–±–∞–≤–∏—Ç—å `nextStates: InputArrays[]`
   - ‚úÖ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å `dones: Float32Array`
2. ‚úÖ –°–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å `SACMemory` –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
3. ‚úÖ –°–æ–∑–¥–∞—Ç—å `SACReplayBuffer` –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è replay buffer
4. ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É `PrioritizedReplayBuffer` (–≥–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é)
5. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥—ã –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –±–∞—Ç—á–µ–π –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö `SACMemoryBatch` —Å–æ–∑–¥–∞–Ω–∞
- ‚úÖ –ö–ª–∞—Å—Å `SACMemory` —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω
- ‚úÖ `SACReplayBuffer` –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ PER –¥–æ–±–∞–≤–ª–µ–Ω–∞

**–§–∞–π–ª—ã:**
- ‚úÖ `packages/ml-common/Memory.ts`
- ‚úÖ `packages/ml-common/SACReplayBuffer.ts`

### –§–∞–∑–∞ 5: Actor Workers (2-3 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/Actor/EpisodeManager.ts`
2. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:
   - ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `SACMemory` –¥–ª—è —Å–±–æ—Ä–∞ (s, a, r, s', done)
   - ‚úÖ –£–±—Ä–∞—Ç—å –≤—ã–∑–æ–≤—ã value network (–Ω–µ –Ω—É–∂–Ω—ã –≤ SAC)
   - ‚úÖ –£–±—Ä–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ GAE/V-trace (–Ω–µ –Ω—É–∂–Ω—ã –≤ SAC)
3. ‚¨ú –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è `done` —Ñ–ª–∞–≥–∞ (—É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–æ–≤–æ–º –∫–æ–¥–µ)
4. ‚¨ú –î–æ–±–∞–≤–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ `next_state` (—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Agent)
5. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `packages/ml/src/SAC/ActorWorker.ts`
6. ‚¨ú –î–æ–±–∞–≤–∏—Ç—å initial random exploration (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –§–∞–∑—ã 9)

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ EpisodeManager —Å–æ–∑–¥–∞–Ω –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è SAC
- ‚¨ú Actor –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (—Ç—Ä–µ–±—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Agent)
- ‚¨ú –ü–µ—Ä–µ—Ö–æ–¥—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Å `next_states` –∏ `dones` (—Ç—Ä–µ–±—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Agent)
- ‚úÖ –ù–µ—Ç memory leaks

**–§–∞–π–ª—ã:**
- ‚úÖ `packages/ml/src/SAC/Actor/EpisodeManager.ts`
- ‚úÖ `packages/ml/src/SAC/ActorWorker.ts`
- ‚úÖ `packages/ml/src/SAC/TODO_AGENT_INTEGRATION.md` (–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤)

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:**
–î–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–∏—Ç—å Agent –∫–ª–∞—Å—Å—ã (CurrentActorAgent, HistoricalAgent).
TODO —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.

### –§–∞–∑–∞ 6: Learner Workers (3-4 –¥–Ω—è) ‚úÖ –ó–ê–í–ï–†–®–ï–ù–û

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/Learner/createActorLearner.ts`
2. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/Learner/createCriticLearner.ts`
3. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `packages/ml/src/SAC/Learner/createLearnerManager.ts`:
   - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è actor –∏ critics
   - Soft target updates
   - Temperature updates (–µ—Å–ª–∏ auto-tune)
4. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/Learner/LearnerActorWorker.ts`
5. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/Learner/LearnerCriticWorker.ts`
6. ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏ –º–µ–∂–¥—É workers –≤ `packages/ml/src/SAC/channels.ts`
7. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö 4 –º–æ–¥–µ–ª–µ–π

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ Workers –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—É—á–∞—é—Ç —Å–≤–æ–∏ –º–æ–¥–µ–ª–∏
- ‚úÖ Soft updates —Ä–∞–±–æ—Ç–∞—é—Ç
- ‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è
- ‚úÖ –ù–µ—Ç race conditions

**–§–∞–π–ª—ã:**
- ‚úÖ `packages/ml/src/SAC/Learner/createActorLearner.ts` - –æ–±—É—á–∞–µ—Ç actor —á–µ—Ä–µ–∑ –º–∏–Ω–∏–º—É–º twin Q-values
- ‚úÖ `packages/ml/src/SAC/Learner/createCriticLearner.ts` - –æ–±—É—á–∞–µ—Ç –æ–±–∞ critic networks —Å soft updates
- ‚úÖ `packages/ml/src/SAC/Learner/createLearnerManager.ts` - –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç replay buffer –∏ learners
- ‚úÖ `packages/ml/src/SAC/Learner/LearnerActorWorker.ts` - worker –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è actor
- ‚úÖ `packages/ml/src/SAC/Learner/LearnerCriticWorker.ts` - worker –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –æ–±–æ–∏—Ö critics
- ‚úÖ `packages/ml/src/SAC/Learner/createLearnerAgent.ts` - –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è learner agents
- ‚úÖ `packages/ml/src/SAC/Learner/isLossDangerous.ts` - –≤–∞–ª–∏–¥–∞—Ü–∏—è loss –∑–Ω–∞—á–µ–Ω–∏–π

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:**
Temperature auto-tuning —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ `trainTemperature()` –Ω–æ –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ learner worker (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ñ–∏—á–∞).

### –§–∞–∑–∞ 7: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ (1 –¥–µ–Ω—å) ‚úÖ –ó–ê–í–ï–†–®–ï–ù–û

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `packages/ml/src/SAC/index.ts` (—Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞)
2. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/sac.html` (–∫–æ–ø–∏—è `appo.html`)
3. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/sac.ts` (–∫–æ–ø–∏—è `appo.ts`)
4. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `packages/ml/config.vite.ts` –¥–ª—è SAC entry point (—É–∂–µ –≥–æ—Ç–æ–≤)
5. ‚úÖ –°–æ–∑–¥–∞—Ç—å `packages/ml/src/SAC/VisTest/VisTestEpisodeManager.ts` –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ SAC –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ `sac.html`
- ‚úÖ Workers —Å–æ–∑–¥–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- ‚ö†Ô∏è UI –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ (–±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç, SAC-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤ –§–∞–∑–µ 9)

**–§–∞–π–ª—ã:**
- ‚úÖ `packages/ml/src/SAC/index.ts` - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç TensorFlow, —Å–æ–∑–¥–∞–µ—Ç actor –∏ learner workers
- ‚úÖ `packages/ml/sac.html` - HTML —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è SAC
- ‚úÖ `packages/ml/sac.ts` - –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç SAC/index.ts
- ‚úÖ `packages/ml/src/SAC/VisTest/VisTestEpisodeManager.ts` - –º–µ–Ω–µ–¥–∂–µ—Ä –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
- ‚úÖ `packages/ml/config.vite.ts` - —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π)

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:**
–î–ª—è –∑–∞–ø—É—Å–∫–∞: `npm run dev` –∏ –æ—Ç–∫—Ä—ã—Ç—å `http://localhost:5173/sac.html`

### –§–∞–∑–∞ 8: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (2-3 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚¨ú –°–æ–∑–¥–∞—Ç—å unit-—Ç–µ—Å—Ç—ã –¥–ª—è:
   - `sampleActionWithReparam()`
   - `trainActorNetwork()`
   - `trainCriticNetworks()`
   - `trainTemperature()`
   - `softUpdateTargetNetworks()`
2. ‚¨ú –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, single agent vs bot)
3. ‚¨ú –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ Q-values –Ω–µ diverge
4. ‚¨ú –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ alpha –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è (–µ—Å–ª–∏ auto-tune)
5. ‚¨ú –û—Ç–ª–∞–¥–∫–∞ NaN/Inf –≤ loss:
   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å gradient clipping
   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å bounds –¥–ª—è log_std
   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å numerical stability –≤ log-prob calculation
6. ‚¨ú –ü—Ä–æ–≤–µ—Ä–∏—Ç—å memory usage (4 networks –º–æ–≥—É—Ç –∑–∞–Ω–∏–º–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏)

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç
- –û–±—É—á–µ–Ω–∏–µ —Å—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–µ
- –ù–µ—Ç NaN/Inf –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
- Memory usage –ø—Ä–∏–µ–º–ª–µ–º—ã–π

**–§–∞–π–ª—ã:**
- `packages/ml/src/SAC/train.test.ts`
- `packages/ml/src/Models/Create.test.ts`

### –§–∞–∑–∞ 9: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (1-2 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚¨ú –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
   - Learning rates (actor, critics, alpha)
   - Batch size
   - Tau (soft update coefficient)
   - Alpha (temperature)
   - Replay buffer size
2. ‚¨ú –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
   - –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è step
   - Memory footprint
   - GPU utilization
3. ‚¨ú –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
   - Batch inference –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
   - –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ tensors
   - –£–º–µ–Ω—å—à–µ–Ω–∏–µ memory allocations

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ä–∞–≤–Ω–∏–º–∞ –∏–ª–∏ –ª—É—á—à–µ PPO
- –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
- –ù–µ—Ç bottlenecks

### –§–∞–∑–∞ 10: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (1-2 –¥–Ω—è)

**–ó–∞–¥–∞—á–∏:**
1. ‚¨ú –ù–∞–ø–∏—Å–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é:
   - –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å SAC
   - –û–ø–∏—Å–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - –û—Ç–ª–∏—á–∏—è –æ—Ç PPO
   - Troubleshooting
2. ‚¨ú A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ PPO vs SAC:
   - –°—Ä–∞–≤–Ω–∏—Ç—å sample efficiency
   - –°—Ä–∞–≤–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
   - –°—Ä–∞–≤–Ω–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
3. ‚¨ú –°–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
4. ‚¨ú –û–±–Ω–æ–≤–∏—Ç—å README.md

**–ö—Ä–∏—Ç–µ—Ä–∏–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞–ø–∏—Å–∞–Ω–∞
- –ï—Å—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã

**–§–∞–π–ª—ã:**
- `packages/ml/README.md`
- `SAC_GUIDE.md` (–Ω–æ–≤—ã–π —Ñ–∞–π–ª)

---

## 8. –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è

### –†–∏—Å–∫–∏

| –†–∏—Å–∫ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –í–ª–∏—è–Ω–∏–µ | –ú–∏—Ç–∏–≥–∞—Ü–∏—è |
|------|-------------|---------|-----------|
| SAC –º–µ–Ω–µ–µ sample-efficient –Ω–∞ —Å—Ç–∞—Ä—Ç–µ | –í—ã—Å–æ–∫–∞—è | –°—Ä–µ–¥–Ω–µ–µ | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PER, —É–≤–µ–ª–∏—á–∏—Ç—å replay buffer |
| –¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ (4 networks) | –í—ã—Å–æ–∫–∞—è | –í—ã—Å–æ–∫–æ–µ | –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å memory, —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä networks –µ—Å–ª–∏ –Ω—É–∂–Ω–æ |
| –°–ª–æ–∂–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ–µ | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å auto-tuning –¥–ª—è alpha, grid search |
| NaN/Inf –≤ –æ–±—É—á–µ–Ω–∏–∏ | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–æ–µ | Gradient clipping, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å numerical stability |
| –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ –∏–∑-–∑–∞ 4 networks | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ–µ | –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å inference, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batch operations |
| –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤ multi-agent —Å—Ä–µ–¥–µ | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–æ–µ | –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (curriculum learning) |

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

1. üîÑ **–°–æ—Ö—Ä–∞–Ω–∏—Ç—å PPO –∫–æ–¥** –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–µ—Ç–∫–µ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ rollback
2. üìä **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** PPO vs SAC –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –º–∏–≥—Ä–∞—Ü–∏–∏
3. üéØ **Auto-tuning alpha** —É–ø—Ä–æ—Å—Ç–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É entropy regularization
4. üíæ **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PrioritizedReplayBuffer** –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
5. üìà **–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**:
   - Q-values (Q1, Q2, min Q, target Q)
   - Alpha (temperature)
   - Policy entropy
   - Actor loss, Critic losses
   - Gradient norms
   - Replay buffer statistics
6. üß™ **Incremental rollout**: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ single-agent, –ø–æ—Ç–æ–º multi-agent
7. üìù **Checkpoint —á–∞—Å—Ç–æ**: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —É—Å–ø–µ—à–Ω–æ–π —Ñ–∞–∑—ã

---

## 9. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ SAC

### –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å PPO

- ‚úÖ **–õ—É—á—à–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π** –±–ª–∞–≥–æ–¥–∞—Ä—è maximum entropy objective
- ‚úÖ **–ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** –±–ª–∞–≥–æ–¥–∞—Ä—è off-policy learning –∏ twin critics
- ‚úÖ **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ** —á–µ—Ä–µ–∑ replay buffer (–∫–∞–∂–¥—ã–π sample –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ)
- ‚úÖ **–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è continuous action spaces** (—á—Ç–æ —É –Ω–∞—Å –∏ –µ—Å—Ç—å)
- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ exploration/exploitation** —á–µ—Ä–µ–∑ temperature parameter Œ±
- ‚úÖ **–ù–µ —Ç—Ä–µ–±—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è advantages** (–ø—Ä–æ—â–µ –∏ –±—ã—Å—Ç—Ä–µ–µ)
- ‚úÖ **–ú–µ–Ω—å—à–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** (—Å auto-tuning alpha)

### –î–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏ (multi-agent tanks)

- ‚úÖ **–õ—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å sparse rewards** –±–ª–∞–≥–æ–¥–∞—Ä—è exploration
- ‚úÖ **–ë–æ–ª–µ–µ robust–Ω–∞—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏** (non-stationary multi-agent environment)
- ‚úÖ **–õ—É—á—à–µ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏** –±–ª–∞–≥–æ–¥–∞—Ä—è maximum entropy policy

---

## 10. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

### –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

1. **Actor metrics:**
   - Actor loss
   - Policy entropy (—Å—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–µ–π—Å—Ç–≤–∏–π)
   - Action mean/std statistics
   - Gradient norms

2. **Critic metrics:**
   - Critic1 loss
   - Critic2 loss
   - Q1 values (mean, min, max)
   - Q2 values (mean, min, max)
   - Target Q values
   - TD error

3. **Alpha (temperature):**
   - Current alpha value
   - Alpha loss (–µ—Å–ª–∏ auto-tune)
   - Target entropy vs actual entropy

4. **Replay buffer:**
   - Buffer size
   - Sampling statistics
   - Priority statistics (–µ—Å–ª–∏ PER)

5. **Episode metrics:**
   - Episode reward
   - Episode length
   - Win rate
   - Survival time

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

- Q-value overestimation (Q1 vs Q2 difference)
- Action distribution visualization
- State-value estimation accuracy
- Sample efficiency (reward per timestep)

---

## 11. –ß–µ–∫–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º

### Pre-flight checks

- [ ] –í—Å–µ 4 —Å–µ—Ç–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
- [ ] Forward pass —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ç–µ–π
- [ ] Soft target update –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∫–æ–ø–∏—Ä—É–µ—Ç –≤–µ—Å–∞
- [ ] Replay buffer —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
- [ ] Actor —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å `next_states` –∏ `dones`
- [ ] Learners –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—É—á–∞—é—Ç —Å–≤–æ–∏ –º–æ–¥–µ–ª–∏
- [ ] –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –≤–∑—Ä—ã–≤–∞—é—Ç—Å—è (NaN/Inf check)
- [ ] Memory usage –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã
- [ ] –í—Å–µ workers –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
- [ ] –ú–µ—Ç—Ä–∏–∫–∏ –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è
- [ ] –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è
- [ ] –ï—Å—Ç—å baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å PPO

---

## 12. –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –ø–æ—Å–ª–µ SAC

–ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ SAC, –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å:

1. **TD3 (Twin Delayed DDPG)** - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è SAC –±–µ–∑ entropy term
2. **Rainbow DQN** - –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è)
3. **Distributed SAC** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
4. **Multi-agent SAC** - —è–≤–Ω—ã–π —É—á–µ—Ç –¥—Ä—É–≥–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ Q-—Ñ—É–Ω–∫—Ü–∏—è—Ö
5. **Hierarchical RL** - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ high-level –∏ low-level –ø–æ–ª–∏—Ç–∏–∫–∏
6. **Meta-learning** - –±—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º

---

**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** 17 –æ–∫—Ç—è–±—Ä—è 2025  
**–°—Ç–∞—Ç—É—Å:** Draft  
**–ê–≤—Ç–æ—Ä:** AI Assistant
